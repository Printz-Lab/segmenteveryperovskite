{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95a8f02b",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ebc518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import segmenteverygrain as seg\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/pjlohr/Documents/GitHub/segmenteverygrain/segmenteverygrain')\n",
    "import segmenteverygrain as seg\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from skimage import measure\n",
    "from skimage.measure import regionprops, regionprops_table\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from importlib import reload\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)\n",
    "gpus\n",
    "\n",
    "# %matplotlib widget\n",
    "# %matplotlib qt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f474ac20",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe1bd2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 15:06:40.749979: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Max\n",
      "2025-07-03 15:06:40.750049: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 36.00 GB\n",
      "2025-07-03 15:06:40.750053: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 13.50 GB\n",
      "2025-07-03 15:06:40.750071: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-07-03 15:06:40.750080: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "`keras.optimizers.legacy` is not supported in Keras 3. When using `tf.keras`, to continue using a `tf.keras.optimizers.legacy` optimizer, you can install the `tf_keras` package (Keras 2) and set the environment variable `TF_USE_LEGACY_KERAS=True` to configure TensorFlow to use `tf_keras` when accessing `tf.keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m seg\u001b[38;5;241m.\u001b[39mUnet()\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, loss\u001b[38;5;241m=\u001b[39mseg\u001b[38;5;241m.\u001b[39mweighted_crossentropy, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/pjlohr/Documents/GitHub/segmenteverygrain/segmenteverygrain/checkpoints/seg_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m sam_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/pjlohr/Documents/GitHub/segmenteverygrain/segmenteverygrain/checkpoints/sam_vit_h_4b8939.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/segmenteverygrain/lib/python3.9/site-packages/keras/src/optimizers/__init__.py:116\u001b[0m, in \u001b[0;36mLegacyOptimizerWarning.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`keras.optimizers.legacy` is not supported in Keras 3. When using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.keras`, to continue using a `tf.keras.optimizers.legacy` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer, you can install the `tf_keras` package (Keras 2) and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset the environment variable `TF_USE_LEGACY_KERAS=True` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigure TensorFlow to use `tf_keras` when accessing `tf.keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m     )\n",
      "\u001b[0;31mImportError\u001b[0m: `keras.optimizers.legacy` is not supported in Keras 3. When using `tf.keras`, to continue using a `tf.keras.optimizers.legacy` optimizer, you can install the `tf_keras` package (Keras 2) and set the environment variable `TF_USE_LEGACY_KERAS=True` to configure TensorFlow to use `tf_keras` when accessing `tf.keras`."
     ]
    }
   ],
   "source": [
    "model = seg.Unet()\n",
    "model.compile(optimizer=Adam(), loss=seg.weighted_crossentropy, metrics=[\"accuracy\"])\n",
    "model.load_weights('/Users/pjlohr/Documents/GitHub/segmenteverygrain/segmenteverygrain/checkpoints/seg_model')\n",
    "\n",
    "sam_checkpoint = \"/Users/pjlohr/Documents/GitHub/segmenteverygrain/segmenteverygrain/checkpoints/sam_vit_h_4b8939.pth\"\n",
    "device = \"mps\"\n",
    "model_type = \"default\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c1dfc6",
   "metadata": {},
   "source": [
    "# Run segmentation\n",
    "\n",
    "### Grains are supposed to be well defined in the image; e.g., if a grain consists of only a few pixels, it is unlikely to be detected.\n",
    "\n",
    "### The segmentation can take a few minutes even for medium-sized images. Images with ~2000 pixels along their largest dimension are a good start and allow the user to get an idea about how well the segmentation works.\n",
    "\n",
    "### If you have a much larger image, see the section **\"Run segmentation on large image\"** at the end of the notebook. Running the `predict_large_image` function takes a lot longer (e.g., several hours), but it is possible to analyze very large images with tens of thousands of grains.\n",
    "\n",
    "### Image used below is available from [here](https://github.com/zsylvester/segmenteverygrain/blob/main/torrey_pines_beach.jpeg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bf3f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# reload(seg)\n",
    "\n",
    "# replace this with the path to your image:\n",
    "fname = '/Users/pjlohr/Documents/GitHub/segmenteverygrain/perovskite_segmentation/rapid/Matt_241113/0.4M/100C/50mmin_20percDMSO_25.4um_gap/center_m003_image.png'\n",
    "\n",
    "data_dir = os.path.dirname(fname)\n",
    "scale_factor = 2\n",
    "\n",
    "image = np.array(load_img(path=fname))\n",
    "\n",
    "# # Scale down image for much faster processing:\n",
    "# image = np.array(load_img(path=fname, \n",
    "#                           color_mode='rgb',\n",
    "#                           target_size=(image.shape[0] // scale_factor, image.shape[1] // scale_factor),\n",
    "#                           interpolation='lanczos'\n",
    "#                           ))\n",
    "\n",
    "print(image.shape)  # Verify the shape (height, width, channels)\n",
    "\n",
    "image_pred = seg.predict_image(image, model, I=256)\n",
    "\n",
    "# decreasing the 'dbs_max_dist' parameter results in more SAM prompts (and longer processing times):\n",
    "labels, coords = seg.label_grains(image, image_pred, dbs_max_dist=1000.0) # Unet prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c091d4c",
   "metadata": {},
   "source": [
    "### Use the figure created in the next cell to check the quality of the Unet labeling (sometimes it doesn't work at all) and the distribution of SAM prompts (= black dots). If the Unet prediction is of poor quality, it is a good idea to create some training data and fine tune the base model so that it works better with the images of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e031a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax.imshow(image)\n",
    "plt.scatter(np.array(coords)[:,0], np.array(coords)[:,1], c='k')\n",
    "plt.xticks([])\n",
    "plt.yticks([]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c992fcf8",
   "metadata": {},
   "source": [
    "# Run SAM segmentation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bcedac-6a3f-4f1d-97a9-b167fa14994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAM segmentation, using the point prompts from the Unet:\n",
    "all_grains, labels, mask_all, grain_data, fig, ax = seg.sam_segmentation(sam, image, image_pred, \n",
    "            coords, labels, min_area=2000.0, plot_image=False, remove_edge_grains=True, remove_large_objects=False)\n",
    "\n",
    "\n",
    "# plot results again if necessary\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax.imshow(image)\n",
    "seg.plot_image_w_colorful_grains(image, all_grains, ax, cmap='Paired')\n",
    "seg.plot_grain_axes_and_centroids(all_grains, labels, ax, linewidth=1, markersize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968737c4",
   "metadata": {},
   "source": [
    "# Delete or merge grains in segmentation result\n",
    "* click on the grain that you want to remove and press the 'x' key\n",
    "* click on two grains that you want to merge and press the 'm' key (they have to be the last two grains you clicked on)\n",
    "* press the 'g' key to hide the grain masks (so that you can see the original image better); press the 'g' key again to show the grain masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1761761",
   "metadata": {},
   "outputs": [],
   "source": [
    "grain_inds = []\n",
    "cid1 = fig.canvas.mpl_connect('button_press_event', \n",
    "                              lambda event: seg.onclick2(event, all_grains, grain_inds, ax=ax))\n",
    "cid2 = fig.canvas.mpl_connect('key_press_event', \n",
    "                              lambda event: seg.onpress2(event, all_grains, grain_inds, fig=fig, ax=ax))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d08448d",
   "metadata": {},
   "source": [
    "Run this cell if you do not want to delete / merge existing grains anymore; it is a good idea to do this before moving on to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6987ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.canvas.mpl_disconnect(cid1)\n",
    "fig.canvas.mpl_disconnect(cid2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ef7d75",
   "metadata": {},
   "source": [
    "Use this function to update the 'labels' array after deleting and merging grains (the 'all_grains' list is updated when doing the deletion and merging):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b8082",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_grains, labels, mask_all = seg.get_grains_from_patches(ax, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ca344c",
   "metadata": {},
   "source": [
    "Plot the updated set of grains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0f7f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "seg.plot_image_w_colorful_grains(image, all_grains, ax, cmap='Paired', plot_image=True)\n",
    "seg.plot_grain_axes_and_centroids(all_grains, labels, ax, linewidth=1, markersize=10)\n",
    "plt.xlim([0, np.shape(image)[1]])\n",
    "plt.ylim([np.shape(image)[0], 0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7201a5",
   "metadata": {},
   "source": [
    "## Add new grains using the Segment Anything Model\n",
    "\n",
    "* click on unsegmented grain that you want to add\n",
    "* press the 'x' key if you want to delete the last grain you added\n",
    "* press the 'm' key if you want to merge the last two grains that you added\n",
    "* right click outside the grain (but inside the most recent mask) if you want to restrict the grain to a smaller mask - this adds a background prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b1a138",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = SamPredictor(sam)\n",
    "predictor.set_image(image) # this can take a while\n",
    "coords = []\n",
    "cid3 = fig.canvas.mpl_connect('button_press_event', lambda event: seg.onclick(event, ax, coords, image, predictor))\n",
    "cid4 = fig.canvas.mpl_connect('key_press_event', lambda event: seg.onpress(event, ax, fig))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02386a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.canvas.mpl_disconnect(cid3)\n",
    "fig.canvas.mpl_disconnect(cid4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae4b89a",
   "metadata": {},
   "source": [
    "After you are done with the deletion / addition of grain masks, run this cell to generate an updated set of grains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4355b526",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_grains, labels, mask_all = seg.get_grains_from_patches(ax, image, plotting=False)\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "seg.plot_image_w_colorful_grains(image, all_grains, ax, cmap='Paired', plot_image=True)\n",
    "seg.plot_grain_axes_and_centroids(all_grains, labels, ax, linewidth=1, markersize=10)\n",
    "plt.xlim([0, np.shape(image)[1]])\n",
    "plt.ylim([np.shape(image)[0], 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768a05d2",
   "metadata": {},
   "source": [
    "## Get grain size distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96892a89",
   "metadata": {},
   "source": [
    "Run this cell and then click (left mouse button) on one end of the scale bar in the image and click (right mouse button) on the other end of the scale bar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869c6cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cid5 = fig.canvas.mpl_connect('button_press_event', lambda event: seg.click_for_scale(event, ax))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658e2f0a",
   "metadata": {},
   "source": [
    "Use the length of the scale bar in pixels (it should be printed above) to get the scale of the image (in units / pixel):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7893cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_of_units = 200 # Units of length on scale bar. Typically use the entire bar for more precision\n",
    "n_of_pixels = 1008\n",
    "units_per_pixel = n_of_units/n_of_pixels # length of scale bar in pixels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56c5732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directories\n",
    "# os.makedirs(training_data_dir + \"/grain_data/\", exist_ok=True)\n",
    "# os.makedirs(training_data_dir + \"/colorful_grains/\", exist_ok=True)\n",
    "# os.makedirs(training_data_dir + \"/masks/\", exist_ok=True)\n",
    "# os.makedirs(training_data_dir + \"/labels/\", exist_ok=True)\n",
    "\n",
    "props = regionprops_table(labels.astype('int'), intensity_image = image, properties =\\\n",
    "        ('label', 'area', 'centroid', 'major_axis_length', 'minor_axis_length', \n",
    "         'orientation', 'perimeter', 'max_intensity', 'mean_intensity', 'min_intensity'))\n",
    "grain_data = pd.DataFrame(props)\n",
    "\n",
    "stats_df = pd.DataFrame()\n",
    "\n",
    "stats_df['area'] = grain_data['area'].values * units_per_pixel**2\n",
    "stats_df['perimeter'] = grain_data['perimeter'].values * units_per_pixel\n",
    "stats_df['major_axis_length'] = grain_data['major_axis_length'].values * units_per_pixel\n",
    "stats_df['minor_axis_length'] = grain_data['minor_axis_length'].values * units_per_pixel\n",
    "stats_df['orientation'] = grain_data['orientation'].values\n",
    "\n",
    "stats_df.to_csv(data_dir + '/' +fname.split('/')[-1][:-4]+'.csv') # save grain data to CSV file\n",
    "# stats_df.head()\n",
    "\n",
    "# Get colorful grains\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "seg.plot_image_w_colorful_grains(image, all_grains, ax, cmap='plasma', plot_image=True, color_by_size=True)\n",
    "plt.xlim([0, np.shape(image)[1]])\n",
    "plt.ylim([np.shape(image)[0], 0])\n",
    "# Save the plot with minimal white space\n",
    "plt.savefig(data_dir + '/' +fname.split('/')[-1][:-4] + '_colorful_grains.png', bbox_inches='tight', pad_inches=0)\n",
    "# write grayscale mask to PNG file\n",
    "cv2.imwrite(data_dir + '/' +fname.split('/')[-1][:-4] + '_mask.png', mask_all)\n",
    "\n",
    "# Define a colormap using matplotlib\n",
    "num_classes = len(all_grains)\n",
    "cmap = plt.get_cmap('plasma', num_classes)\n",
    "# Map each class label to a unique color using the colormap\n",
    "vis_mask = cmap(labels.astype(np.uint16))[:,:,:3] * 255\n",
    "vis_mask = vis_mask.astype(np.uint8)\n",
    "\n",
    "# Save the colored labels as a PNG file\n",
    "cv2.imwrite(data_dir + '/' + fname.split('/')[-1][:-4] + '_labels.png', vis_mask)\n",
    "\n",
    "# Save the image as a PNG file\n",
    "cv2.imwrite(data_dir + '/' + fname.split('/')[-1][:-4] + '_image.png', cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25703e4-2a4e-4e98-aa49-ff9b816b2ec0",
   "metadata": {},
   "source": [
    "### Finetuning the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f62c8-be0f-47fe-b660-2c55e2d51d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # patchify images and masks\n",
    "# input_dir = \"./Masks_and_images/\" # the input directory should contain files with 'image' and 'mask' in their filenames\n",
    "# patch_dir = \"./New_project/\" # a directory called \"Patches\" will be created here\n",
    "# image_dir, mask_dir = seg.patchify_training_data(input_dir, patch_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f98137-2dfd-4b88-8dc5-e4e79fa343ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create training, validation, and test datasets\n",
    "# train_dataset, val_dataset, test_dataset = seg.create_train_val_test_data(image_dir, mask_dir, augmentation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43650ad9-c2e5-4fc2-856f-4f39b8f1d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load base model weights and train the model with the new data\n",
    "# weights_dir = './checkpoints/seg_model'\n",
    "# model = seg.create_and_train_model(weights_dir, train_dataset, val_dataset, test_dataset, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41375c1f-5f16-496f-b5e8-497626eb89dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save finetuned model as new model (this then can be loaded using \"model.load_weights('./new_model/seg_model')\"\n",
    "# model.save_weights('./new_model/seg_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segmenteverygrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
